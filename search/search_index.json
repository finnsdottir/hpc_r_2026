{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"R and RStudio on the Cluster.","text":"<p>Welcome to this workshop on using R and RStudio on the Digital Research Alliance of Canada's high-performance computing clusters. Over the course of this workshop, we will log into a cluster, load R and download some packages, and then use RStudio to do some model testing and visualization. Afterwards, we will submit our full script as a batch job to the cluster. </p> <p>This session is suitable for those with some R experience, who feel comfortable with writing and running R scripts. No prior knowledge of Linux or high-performance computing is required. While the examples and data used will be aimed at the Humanities and Social Sciences community, the session is open to anyone and everyone interested in learning about R.</p>"},{"location":"#learning-outcomes","title":"Learning outcomes","text":"<ol> <li>Learn to use access the Digital Research Alliance of Canada's HPC clusters</li> <li>Learn to navigate the cluster in your terminal</li> <li>Learn to load R in the terminal, and to access RStudio through JupyterHub</li> <li>Learn to submit a job to the cluster</li> </ol>"},{"location":"#before-we-start","title":"Before we start","text":"<p>Using the Alliance clusters.</p> <p>To use the Digital Research Alliance of Canada's clusters, you will need to first sign up for an account. To sign up for an account with the Alliance, follow the instructions here. </p> <p>We will be using a subset of data from the National Longitudinal Survey of Older Men, 1966-1990. You can download it here. </p> <p>I'd encourage you to follow along in R and RStudio, but if you get stuck or want to check something, you can download my RStudio script, my R script for the shell, and my job script. </p> Further resources. <ul> <li>High-performance research computing in R</li> <li>R for Graduate Students</li> <li>Getting Started (on HPC)</li> <li>How to run your R script with Compute Canada</li> <li>Alliance's documentation on R</li> <li>Managing R on HPC systems</li> <li>Simple Linear Regression</li> </ul> <p>Instructor: Maria Sigridur Finnsdottir, PhD.  </p>"},{"location":"analysing_rstudio/","title":"Analysing data in RStudio.","text":"<p>Now that we have our RStudio set up, we need to upload our data. There are several ways to upload data into the cluster under your account:</p> <ol> <li>Data can be moved into the cluster using Globus. Here's a video explaining how.</li> <li>You can import data directly into JupyterHub in your broswer. After launching a server, with the JupyterLab user interface, select the upload files button and open the desired file in your finder. </li> <li>Using a secure copy protocol, or <code>stp</code>.</li> </ol> <p>The <code>scp</code> command follows the structure <code>scp &lt;file start location&gt; &lt;file end location&gt;</code> whether you are moving data on to the remote server from your computer, or from the remote server on to your computer. When the file location is on the remote server, it must start with <code>&lt;username&gt;@&lt;remote server address&gt;:</code> and then the file location on the server. </p> <p>You should already have the nlsc_data.csv downloaded. My downloaded data is in my downloads folder, at the file path <code>./Downloads/nlsc_data.csv</code>. I plan to save it in my home directory on the cluster (represented by the tilde ~). So, to move the data, run the following command (replacing the username and file paths as appropriate): </p> <p><pre><code>scp \"./Downloads/nlsc_data.csv\" finnsdot94@feb2026-uofa.c3.ca:~/\n</code></pre> The terminal will then prompt you to enter your password for the cluster. Once you've done that, you should see a printout of your file transfer progress.</p> <p>Once that is complete, you can navigate back to your RStudio session in your browser. Your data should now be visible in the <code>Files</code> tab in the lower right-hand pane.  </p> About the data. <p>In this workshop, we're using data from the National Longitudinal Survey of Older Men, 1966-1990. You can download it here. Specifically, we'll be working with the following variables: </p> <ul> <li>ID</li> <li>Age</li> <li>Race</li> <li>Marital status</li> <li>Urban/rural residence</li> <li>Highest grade attained</li> <li>Housing </li> <li>Family income</li> <li>Children</li> </ul> <p>Here is a codebook of the data excerpt we're using.</p> <p>Citation: Bureau of Labor Statistics, U.S. Department of Labor. National Longitudinal Survey of Older Men, 1966-1990 (rounds 1-13). Produced and distributed by the Center for Human Resource Research (CHRR), The Ohio State University. Columbus, OH: 1999.</p> <p>Using this data, we're going to investigate the age-old relationship between education and income. To do this, we're going to run a linear regression of income on education, controlling for various sociodemographic factors. But first, we need to set up our working environment. Once in RStudio, start by setting your working directory to be the <code>R</code> folder, and by creating output folders for your data and figures. </p> <pre><code>getwd()\nsetwd('R')\ngetwd()\n\n#setting up folders\ndir.create('data_output')\ndir.create('fig_output')\ndir.create('table_output')\n</code></pre> <p>If not all your new folders show up when you run the lines, try refreshing the <code>Files</code> pane. </p> <p>We'll save our model output, modified data, and plots in these folders. Next, load the libraries we'll be using. Unlike with the <code>install.packages</code> function, we cannot feed libraries in as a list. </p> <pre><code>library(tidyverse)\nlibrary(naniar)\nlibrary(ggplot2)\nlibrary(stats)\nlibrary(coefplot)\nlibrary(broom)\n</code></pre> <p>After loading the various libraries we'll use, load in your data as \"data\" and inspect it. Since our data is in home directory, and we are working in the R folder, you'll need to start your file path with <code>..</code> to represent the directory above. </p> <p>One option for inspecting our data is to use the <code>str()</code> function, which will return informaiton on the structure of the data frame, as well as on the class, length, and content of each column (variable). You can also call <code>head()</code> and <code>tail()</code> to look at the first and last six rows of the data frame. </p> <pre><code>data &lt;- read.csv(\"../nlsc_data.csv\")\nstr(data)\nhead(data)\n</code></pre> <p>Next, call <code>View(data)</code> and scroll through the data window that pops up. Remember: R is case sensitive, so you must type <code>View</code>, not <code>view</code>. </p> <p>You should see that our data includes some values that don't make sense, such as income values of -2 and -4. This is because missing values are being represented as -2 and -4 in our dataset. Before we do any more analyses, we'll need to replace those with <code>NA</code> values. To do this, we will use functions from the <code>naniar</code> package. First, let's replace the missing values in the income variable. We're going to assign our data to a new data frame, just this one time, to preserve the original data. </p> <pre><code>data2 &lt;- data %&gt;% replace_with_na(replace = list(income = c(-2,-4)))\n</code></pre> <p>In the code above, we supply arguments to the <code>replace_with_na</code> function that specify the variable to be edited and the values to be replaced. We then pipe that function into the data frame, and assign it to a new data frame (to preserve the original data).</p> <p>The <code>naniar</code> package also has a function that allows us to replace a particular value with <code>NA</code> across an entire data frame:</p> <pre><code>data2 &lt;- data2 %&gt;% replace_with_na_all(condition = ~.x == -4)\n</code></pre> <p>In this code, we use the function <code>replace_with_na_all</code> and supply it with the conditional argument replacing any variable (represented by ~.x) value of -4 with <code>NA</code>. We then pipe it into the same modified data frame. </p> <p>Next, let's make a scatterplot from our cleaned data to visually assess the relationship between education and income using <code>ggplot</code>. Let's also save it to our fig_output folder. </p> <p><pre><code>data2 %&gt;%\n  ggplot(aes(x=highest_grade, y=income))+geom_point()\n\nggsave(\"./fig_output/educ_inc_plot.png\")\n</code></pre> The resulting scatterplot shows us that there is a clear, positive relationship between total family income and educational attainment.</p> Scatterplot of income by education. <p>Let's further test this relationship by looking at the correlation between these two variables. We can do this using the <code>cor()</code> function and supplying it with arguments for our <code>x</code> and <code>y</code> variables. Given that we have missing data in our data frame, we will also need to tell it to use only complete observations, otherwise the function will return a value of <code>NA</code>. Remember that you can also go to the \"Help\" tab in RStudio to learn more about individual packages or functions. </p> <pre><code>cor(data2$income, data2$highest_grade, use='complete.obs') \n</code></pre> <p>This function returns a correlation of 0.380902, indicating a moderate positive relationship between education and income - just as we saw in the scatterplot. </p> <p>Taking this another step further, let's now run a linear regression of income on education to test the statistical significant of the relationship. We do this with the <code>lm()</code> function in the <code>stats</code> package. The basic structure of a linear regression in R is <code>model &lt;- lm(x ~ y, data=data)</code>. </p> <pre><code>model.1 &lt;- lm(income ~ highest_grade, data=data2)\nsummary(model.1)\n</code></pre> <p>What do the results tell us about the relationship between income and education? </p> <p>Obviously, the relationship between these things is more complicated. So, let's try adding some control variables to see how race, age, rural residence, and marital status might influence the effect of education on income. Before we do that, we'll need to recode some of our categorical variables into dummy variables. We can do this using the <code>mutate</code> function in <code>dplyr</code>, which allows us to preserve the original variable. First, let's recode race and marital status into dummies for white and married:</p> <pre><code>data2 &lt;- data2 %&gt;%\n  mutate(white = recode(race,\n                        1==1,\n                        2==0,\n                        3==0))\n\ndata2 &lt;- data2 %&gt;%\n  mutate(married = recode(marital,\n                          1==1,\n                          2==1,\n                          3==0,\n                          4==0,\n                          5==0, \n                          6==0))\n</code></pre> <p>As shown above, <code>mutate</code> can be combined with a <code>recode</code> function to recode the values of a variable one by one. However, this can be a bit cumbersome when dealing with a variable with multiple possible values. In these cases, <code>case_when</code> comes quite in handy. Let's now use <code>case_when</code> to recode residence and children. In this case, we'll recode them to test whether rural residence and having a large family impact the relationship between education and income.</p> <pre><code>data2 &lt;- data2 %&gt;%\n  mutate(rural = case_when(\n    residence == 8 ~ 1,\n    TRUE ~ 0))\n\ndata2 &lt;- data2 %&gt;%\n  mutate(child5 = case_when(\n    children &gt;= 5 ~ 1,\n    TRUE ~ 0))\n</code></pre> <p>Now, we can put these into our model. We can build on the basic linear model structure in R by adding control variables behind the independent variable.</p> <pre><code>model.2 &lt;- lm(income ~ highest_grade + age + married + white + rural + child5, data=data2)\nsummary(model.2)\n</code></pre> <p>What does this regression tell us about the sociodemographic factors influencing the effect of education on income? </p> <p>Unfortunately, the way R naturally displays regression output is quite messy. We can produce a much nicer output using the <code>tidy</code> function from the <code>broom</code> package. Let's call <code>tidy</code> on our second model, and then save that output as a csv file to our data output folder.</p> <pre><code>tidy_model &lt;- tidy(model.2)\ntidy_model\n\nwrite.csv(tidy_model, \"./table_output/tidy_lm_model.csv\")\n</code></pre> <p>Let's plot out the coefficients now, using the <code>coefplot</code> package. We can save this plot using the 'Export' tool under the Plots tab in RStudio. </p> <p><pre><code>coefplot(model.2, intercept=FALSE)\n</code></pre> Let's now save our modified data, and our R script. You can save your script in the browser window, and run the following to save your data:</p> <pre><code>write.csv(data2, \"./data_output/modified_nlsc_data.csv\")\n</code></pre>"},{"location":"batch_jobs/","title":"Submitting an R batch job","text":"<p>Now that we have our R script written for a cluster job, we need to write a script that runs that job. We'll use <code>nano</code> again here. </p> <p>The first step in writing your job script is to define the script interpreter (basically, telling the shell what language we'll be using):</p> <pre><code>#!/bin/bash\n</code></pre> <p>In this case, we're telling the shell that we're coding in bash. </p> <p>Next, we need to define the requests in our job so that the scheduler knows how much memory, how much time, and how much compute (CPUs or GPUs) we need. Here, since we're running a pretty small job, we'll just ask for 3 minutes, 1GB of memory, and 1 CPU. We'll also give our job a name so that it's easy to find in the queue. </p> <pre><code>#SBATCH --time=00:03:00\n#SBATCH --mem_per-cpu=1G\n#SBATCH --cpus-per-task=1\n#SBATCH --job-name=\"marias_lm\"\n</code></pre> <p>While you do need to specify how much time you need for every job, you can choose not to specify your memory or compute needs. In those cases, you will be alloted a default amount by the system (256MB per core of memory, and one core). You would only ever use more than one core if your script was set up to run in parallel. </p> <p>For a full list of the options you can use to describe your job with SBATCH, you can run the following command <code>sbatch --help</code>. You can also find more information on setting up and running jobs in the Alliance documentation. </p> <p>The last thing we need to do in our job script is to load an R module and run our R script. If your R script is not in the same folder as your job script, you'll need to add the file path. </p> <p><pre><code>module load r/4.5.0\nRscript shell_script.R\n</code></pre> To save your script, press Cmd+O (on mac) or Ctrl+O (on windows). It will prompt you to name the file and save it. I've named mine <code>shell_job.sh</code>. Then you can press Cmd+X (on mac) or Ctrl+X (on windows) to close the text editor. </p> <p>Next, to run your job, call <code>sbatch</code> followed by the same of your job script. When your job is submitted, there should be a line of output printed to the console with the name of your job, like this: </p> <pre><code>sbatch shell_job.sh\nSubmitted batch job 19\n</code></pre> <p>While it is running, you can watch its progress by calling <code>squeue</code> followed by your username, like so:</p> <pre><code>squeue -u finnsdot94\n</code></pre> <p>This command will print out information about your job, including:</p> <ul> <li>the job ID,</li> <li>the job name,</li> <li>the state of the job (R for running, PD for pending, and CG for completing),</li> <li>and the time remaining on the job.</li> </ul> <p>If you just call <code>squeue</code> alone, you'll get a list of all the jobs in the queue. For </p> <p>Once your job is finished running, it will produce an output file called \"slurm-##.out\". You can open this file using <code>nano</code>. In it, you will find information about the status of your job (for example, where it failed if it did) as well as any output you printed from the R console.  </p>"},{"location":"batch_jobs/#monitoring-and-evaluating-a-job","title":"Monitoring and evaluating a job.","text":"<p>Determining how much time and memory you need for a job is, to some extent, a trial and error sort of thing. (Remember, unless you're doing parallel processing, you're only using one core). Luckily, there are ways to measure how much memory your job is using, allowing you to refine your requests. </p> <p>It is generally good practice to request only as much as you need. Smaller jobs tend to run faster, and less wasteful jobs make the system run smoother for everyone. </p> <p>One way to tell if your job needed more memory or time than it had is to check the slurm output file. If you ran out of memory, you will find an \"oom-kill\" error. If you included printed checkpoints (like we did) you can also use those to judge whether your job needed more time than you give it. </p> <p>You can also use the command <code>sacct</code> after your job is done running to evaluate it. <code>saact</code> displays account data for all jobs and job steps in the log (for more on the command, check out the documentation). Let's try running it now to see how much time our job took and how much memory it used: </p> <pre><code>sacct -o JobID,MaxRSS,Elapsed -j 19.batch\n</code></pre> <p>In the above block of code, <code>-o</code> refers to our request for format. In this case, we are only asking for the job id, the memory used, and the time elapsed - in that order. We then use the argument <code>-j</code> to specify the job, and follow it with our job id. </p> <p>This command produces the following output in the shell:</p> <pre><code>JobID            MaxRSS    Elapsed \n------------ ---------- ---------- \n19.batch        187932K   00:01:06 \n</code></pre> <p>According to this, our job used 187932KB of memory (or, 187.8MB) and only 1 minute and 6 seconds. Based on this, next time we run this or a similar job, we should decrease our memory and time requests. </p> <p>For a full list of the formatting options, run <code>sacct --helpformat</code>.</p> Evaluate your job while it is running <p>You can also check currently running jobs by using the <code>sstat</code> command followed by your job ID. You will also need to specify what information you want and how you want it formatted (otherwise it will print out a huge amount of data on your job), For example, to ask it to display how much memory is being used by job 19 in real time:</p> <p><pre><code>sstat 19.batch --format=\"JobID,MaxRSS\" \n</code></pre> The code above specifies that this is job 19 and asks for the output to be given in the format of job id and then memory, like so:</p> <pre><code>JobID            MaxRSS \n------------ ---------- \n19.batch        188044K \n</code></pre> <p>For a complete list of what options you can use with the <code>sstat</code> command, run the following:</p> <pre><code>sstat --help\n</code></pre>"},{"location":"batch_jobs/#downloading-our-output","title":"Downloading our output.","text":"<p>Now that we've run our analyses, let's quickly check that our outputs were produced and are in the right places. We should have two csv tables with our tidy linear regression outputs in the <code>table_output</code> folder, cleaned data in our <code>data_output</code> folder, and a coefficient plot in our <code>fig_output</code> folder. </p> <p>To look inside the folders without having to change directories, we can use the <code>ls</code> command followed by the subfolder name:</p> <p><pre><code>ls fig_ouput\n</code></pre> Check all three folders and make sure that all your output saved properly. Once you are sure that it did, you can close your remote session. To do so, simply use the command <code>exit</code>.</p> <p>Now that we are working locally, we can download our files from the remote server. We'll do this with <code>scp</code> (secure copy program). First, we need to call <code>scp</code> followed by arguments that describe:</p> <ul> <li>Our username in the remote system</li> <li>The address of the remote system</li> <li>The location and name of the file we want to download</li> <li>The location we want the file to be downloaded to</li> </ul> <p>Let's try this first with our coefficient plot, inside the <code>fig_output</code> folder. Let's save it to our current working directory by using <code>.</code> as the location. </p> <pre><code>scp finnsdot94@feb2026-uofa.c3.ca:R/fig_output/coef_plot.png .\n</code></pre> <p>In the code above, I first call <code>scp</code>, then enter my username <code>@</code> the remote server's address. The name and location of the file come next, after a colon <code>:</code>. Finally, I've used a period <code>.</code> to tell the computer to save the file in my current working directory. You can also put a file path in place of the period to save your files elsewhere. </p> <p>Once you've pressed enter on that command, you will be prompted to enter your password. Once you've succesfully connected, you should see the status of your download pop up in the shell. </p> <p>We also have the option to download and save more than one file at a time using wildcards <code>*</code>. Let's use the wildcard character to download both csv files from our <code>table_output</code> folder at the same time. </p> <pre><code>scp \"finnsdot94@feb2026-uofa.c3.ca:R/table_output/*.csv\" .\n</code></pre> <p>You should notice that, in addition to using a wildcard instead of a file name, we've also had to enclose our username, address, and file request in quotation marks. </p> <p>Finally, use <code>scp</code> to download our cleaned/modified data file. </p> Solution. <pre><code>scp finnsdot94@feb2026-uofa.c3.ca:R/data_output/modified_nlsc_data.csv .\n</code></pre>"},{"location":"land_acknowledgement/","title":"Land Acknowledgement","text":"<p>Before we start the lesson, let's take a moment to think about the land on which we are learning and teaching. </p> <p>I'm teaching this lesson from Edmonton, Alberta. The city of Edmonton is located on Treaty Six Territory. </p> <p>This territory has been a home, a trading post, a travelling route, and more for generations of Indigenous Peoples. In particular, the n\u00eahiyaw (Nay-hee-yow) / Cree, Dene (Deh-neyh), Anishinaabe (Ah-nish-in-ah-bay) / Saulteaux (So-toe), Nakota Isga (Na-koh-tah ee-ska) / Nakota Sioux (Na-koh-tah sue), and Niitsitapi (Nit-si-tahp-ee) / Blackfoot peoples have lived, learned, and work in this place for centuries. This land is also a homeland for the M\u00e9tis, and the home of one of the largest communities of Inuit south of the 60th parallel. </p> <p>In this map from native-land.ca you can see a pin representing Edmonton.</p> Edmonton is located within Treaty 6 Territory <p>Land acknowledgements are just one small piece of the work of reconciliation and must be undertaken in conjunction with other works. Acknowledging the land provides a foundation for recognizing Indigenous sovereignty and self-determination, for respecting Indigenous culture and language, and for committing to reconciliation and allyship. </p>"},{"location":"rscripts/","title":"Using R in the Shell","text":"<p>Next, we're going to run the same analysis on income and education in R but as a job on the clusters rather than in an interactive RStudio session. Running an R job requires writing two scripts: an R script with our analysis, and then a job script describing our batch job needs. </p> <p>Let's start by navigating the terminal, and making sure that we're still connected to the cluster. If your connection has timed out, reconnect with ssh. </p> <p>R packages</p> <p>Just like when we used RStudio, we cannot install any R packages in batch jobs on the clusters. So, any packages we will call as libraries in our script must be already installed through an interactive session, just like we did before. </p> <p>As a quick reminder, the steps are:</p> <ol> <li>Log into the cluster with SSH</li> <li>Start an R session</li> <li>Install your required packages with <code>install.packages()</code> </li> <li>Answer yes when it prompts you to save locally</li> </ol> <p>Once in the cluster, navigate to the R folder, where our data is saved. </p> <pre><code>[finnsdot94@login1 ~]$ cd R\n[finnsdot94@login1 R]$\n</code></pre> <p>Call the <code>ls</code> command. We should see our <code>nlsc_data.csv</code> file, the three folders we created in R studio, and the script that we saved there. We're now going to create our new R script in this same directory. To do that, use the command <code>nano</code> to open a text editor in the terminal application. We will write our code directory in this window. </p>"},{"location":"rscripts/#coding-for-the-cluster","title":"Coding for the cluster.","text":"<p>It is crucial to include checkpoints in your script when sending in jobs to a cluster. There is a certain amount of guess work in choosing the amount of memory and compute you need, and so, if you guess wrong and your jobs ends before your analyses are done, you don't want to have to repeat work, wasting valuable compute. </p> <p>We're going to build checkpoints in by (1) saving our work after every major step, (2) checking whether the file already exists before starting work. Building in these checkpoints has the added bonus of not needing to update our script every time we run it. To do this, we'll be using the <code>file.exists()</code> function from base R. We can also add print statements after the major steps that we can use to debug if needed. </p> <p>We'll need to start, like always, with loading out libraries. Afterwards, we'll check if our modified dataset is available, and, if not, we'll create it like we did in RStudio. (Feel free to copy and paste this code, rather than typing it all out in the nano text editor).</p> <pre><code>library(tidyverse)\nlibrary(naniar)\nlibrary(ggplot2)\nlibrary(stats)\nlibrary(coefplot)\nlibrary(broom)\n\nif (file.exists(\"./data_output/modified_nlsc_data.csv\")){\n  data &lt;- read.csv(\"./data_output/modified_nlsc_data.csv\")\n} else {\n    data &lt;- read.csv(\"../nlsc_data.csv\")\n    data &lt;- data %&gt;% replace_with_na_all(condition = ~.x %in% c(-2,-4))\n    data &lt;- data %&gt;%\n        mutate(white = case_when(\n        race == 1 ~ 1,\n        TRUE ~ 0))\n    data &lt;- data %&gt;%\n        mutate(married = case_when(\n        marital == 1 ~ 1,\n        marital == 2 ~ 1,\n        TRUE ~ 0))\n    data &lt;- data %&gt;%\n        mutate(rural = case_when(\n        residence == 8 ~ 1,\n        TRUE ~ 0))\n    data &lt;- data %&gt;%\n        mutate(child5 = case_when(\n        children &gt;= 5 ~ 1,\n        TRUE ~ 0))\n    write.csv(data, \"./data_output/modified_nlsc_data.csv\")\n}\n\nprint(\"data cleaning complete.\")\n</code></pre> <p>In the above codeblock, we start by checking if our <code>modified_nlsc_data.csv</code> file exists in the <code>./data_output/</code> folder. If it does, it gets read in to our <code>data</code> data frame. If it does not, then the program moves down to the code cleaning the data and saving it. This means that, if our data does not exist already, it will be created just like we want it, but if it does already exist, then we won't waste any time or compute re-running the data cleaning code.</p> <p>Next, let's run our simple linear regression model, where we test the effect of educational attainment on income without any control variables. Let's also add a <code>Sys.sleep()</code> function to slow our script down and give us time to check it later on. </p> <pre><code>if (! file.exists(\"./table_output/simple_lm.csv\")){\n    model.1 &lt;- lm(income ~ highest_grade, data = data)\n    tidy_model.1 &lt;- tidy(model.1)\n    write.csv(tidy_model.1, \"./table_output/simple_lm.csv\")\n}\n\nprint(\"model 1 complete.\")\n\nSys.sleep(120)\n</code></pre> <p>You'll notice that we're not using the <code>file.exists()</code> function in a slightly different way this time. This is because we don't need to read the file in if it exists. So, instead we've added a NOT operator to the function (!), so that the code block only runs if the file does not exist. </p> <p>Finally, let's run our full linear regression model (the one with the control variables) and produce a coefficient plot based on that model. </p> <pre><code>if (file.exists(\"./table_output/full_lm.csv\")){\n    model.2 &lt;- read.csv(\"./table_output/full_lm.csv\")\n} else {\n    model.2 &lt;- lm(income ~ highest_grade + age + married + white + rural + child5, data=data)\n    tidy_model.2 &lt;- tidy(model.2)\n    write.csv(tidy_model.2, \"./table_output/full_lm.csv\")\n}\n\nprint(\"model 2 complete.\")\n\nif (! file.exists(\"./fig_output/shell_coefplot.png\")){\n    coef_plot &lt;- coefplot(model.2, intercept = FALSE)\n    ggsave(\"./fig_output/shell_coefplot.png\", plot=coef_plot)\n}\n\nprint(\"plot complete.\")\n</code></pre> <p>Here, we check if the full model already exists, and if it does, we read it in to use it for the coefficient plot. If it does not, we run it and save it in our table output folder.</p> <p>You can save your script by first pressing Cmd+O (on mac) Ctrl+O (on windows) (the letter, not the number), which will prompt you to give the file a name and write it out, and then Cmd+X (on mac) or Ctrl+X (on windows) to close the window. I've called my script <code>shell_script.R</code>. You must use the <code>.R</code> ending to indicate that this is an R script. </p> <p>Should you need to edit your script, you can call it back by running <code>nano</code> followed by the file name. </p> <p>Important things to remember!</p> <p>When running a job on a remote server/cluster, make sure that:</p> <ul> <li>All your file paths are relative</li> <li>All your data is already in the cluster</li> <li>All your R packages are already installed</li> <li>You have checkpoints throughout your script</li> </ul>"},{"location":"rstudio/","title":"Using RStudio on HPC.","text":"<p>There are two ways to use R on the Alliance's HPC clusters. The first, and easiest, way is to access RStudio through JupyterHub in your browser window. This is a great way to test code, run visualizations, and do other light work, but bigger jobs should be submitted through the batch job system. </p> <p>To access RStudio, start by locating the web address of the cluster you want to log in to (you can find the various URLs in the Alliance documentation). For example, if we want to use RStudio on Fir, we need to navigate to <code>https://jupyterhub.fir.alliancecan.ca/</code></p> <p>From there, you'll need to log in with your Alliance credentials and then start a server session. On the JupyterHub landing page, you'll have options for how long you want your session to last, how many cores you need, and how much memory you want. You will also be able to specify the GPU configuration and the user interface. </p> <p>After you've started your session, if this is the first time you've used RStudio on this cluster, you'll need to load your chosen RStudie and R modules. Navigate to the menu on the left side of the screen, and click on the hexagon icon. From there, you can scroll down through the available modules and select <code>r/4.5.0</code> and <code>rstudio-server/4.5</code>. Finally, click on the RStudio button that should now have appeared under the Notebook heading. </p> <p>By default, there are only a small number of packages installed in RStudio on the clusters. The first time you log into RStudio on a cluster, you will need to install the packages you use through the shell/terminal (see more here). As the clusters are not hooked up to the internet, you cannot do this within RStudio. </p> <p>Start by logging into the cluster you're using through the shell. Then, start an R session and choose the version of R that you've loaded in RStudio. </p> <p>Alternatively, if you already now which version of R you want to load, you can run it as follows:</p> <pre><code>[finnsdot94@login1 ~]$ module load r/4.5.0\n</code></pre> <p>Once that is loaded up, you'll be able to install the required packages the normal way, by calling <code>install.packages()</code>. For this workshop, you'll need to install <code>tidyverse</code>, <code>broom</code>, <code>coefplot</code>, and <code>naniar</code>:</p> <pre><code>install.packages(c('tidyverse', 'broom', 'coefplot', 'naniar'))\n</code></pre> <p>The first time you install a package through the shell, R will ask whether you want to create a personal library in your home directory - answer <code>yes</code>. You will then have to select a CRAN mirror to use (I generally use Canada (ON1) but you can pick whichever).</p> <p>Once the package is installed (which may take several minutes) you'll need to restart your RStudio session in JupyterHub. Once you've done that, you should see <code>tidyverse</code> in your list of available packages in the packages pane. </p>"},{"location":"terminal/","title":"Crashcourse: Using the shell.","text":"<p>Before we dive into how to do analyses using R on  high-performance computing clusters, let's start with a crashcourse in HPC and the shell.</p>"},{"location":"terminal/#what-is-high-performance-computing","title":"What is high-performance computing?","text":"<p>High-performance computing (also known as supercomputing) is a form of computing that uses multiple, linked, powerful CPUs/GPUs to handle fast, more intense, and larger processing. The principle tool for HPC is the computing cluster. </p> <p>Key terms:</p> <ul> <li>Core: One processing unit (CPU or GPU)</li> <li>Node: basically a computer (processors, memory, hard disc...) without all the accessories</li> <li>Cluster: collection of many connected nodes</li> <li>Rack: The box holding the cluster together</li> </ul> <p>The Digital Research Alliance of Canada provides access to HPC clusters to Canadian researchers free of charge. For more information about how to access these and other resources, please visit the Alliance's documentation. </p>"},{"location":"terminal/#using-the-shell","title":"Using the shell.","text":"<p>Basically, the shell is the program that lets you interact with the computer behind the click and point interface we all use. The application we use to access the shell differs across operating systems:</p> <ul> <li>On Mac and linux machines, use the 'Terminal' to access the shell</li> <li>On newer Windows computers (version 10 or above), you can access the shell through the 'Command Prompt' application</li> <li>On older Windows computers, you will need to download specialized software to access the shell, such as the free version of MobaXterm. </li> </ul> <p>Once you open the terminal, you will seem the command prompt. The prompt itself can be customized, and will look different based on your operating system. It is called a prompt because it is waiting (or, prompting) you to enter a command. </p> My command prompt shows my username and computer name, followed by my working directory (blank) and the % symbol.  <p>To use the shell, we type in commands, which the program then interprets. For example, if we type in <code>whoami</code>, it returns our username:</p> <pre><code>(base) mariafinnsdottir@JRP93PG4V5 ~ % whoami\nmariafinnsdottir\n</code></pre> What is happening here? <ol> <li>The terminal takes this string of characters \u201cwhoami\u201d and looks in a special set of directories on your computer called the PATH for any files that match the string</li> <li>If it finds a match, run the file as a program</li> <li>whoami is recognized as a program</li> <li>That program then returns a result based on how it was programmed</li> </ol> <p>If, however, we type a command that the program does not recognize, it will return an error.   </p> <pre><code>(base) mariafinnsdottir@JRP93PG4V5 ~ % whereami\nzsh: command not found: whereami\n</code></pre> <p>The correct way to find 'where we are,' so to speak, is to print the working directory. We do this using the <code>pwd</code> command. </p> <p>Mac and Linux users will see the <code>pwd</code> command return the current working directory:</p> <pre><code>(base) mariafinnsdottir@JRP93PG4V5 ~ % pwd\n/Users/mariafinnsdottir\n</code></pre> <p>Windows users will get a 'command not found' error here as well, because the original operating system programmers named the command <code>cd</code> (current directory) instead. If you\u2019re working on a Windows computer, type in <code>cd</code> now, and the shell should return your current working directory.</p> <p>Even with the differences between operating systems, typically, you can do the same things. Once we log into the remote system, everyone will see the same thing and will use the same commands.</p> <p>Note</p> <p>You can navigate through your previous commands using the up and down arrow buttons. You can also view a list of previous commands by running the command <code>history</code>.</p>"},{"location":"terminal/#logging-into-the-hpc-cluster","title":"Logging into the HPC cluster.","text":"<p>In order to log into one of the Alliance's clusters, we need to use a secure shell remote login client; the command for this is <code>ssh</code>. The basic structure of the command is: </p> <pre><code>ssh username@remote_system_url\n</code></pre> <p>The arguments in this command are (1) the username you've been assigned in the remote system, and (2) the URL/address of the system you are trying to connect to. To log in to an Alliance cluster, you will use your CCDB username and the address of one of the clusters. For example, if user000 were to log in to the Fir cluster, they would use the following ssh command: </p> <pre><code>ssh user000@fir.alliancecan.ca\n</code></pre> <p>For step-by-step video on logging into the Alliance's clusters, click here. </p> <p>The first time you log in to a system, you will get a warning message with a fingerprint challenge alerting you that your computer doesn't recognize the key fingerprint of the remote system. For example, here is a fingerprint challenge I encountered logging into a workshop cluster for the first time: </p> <p>Reasons you might see a fingerprint challenge when logging in:</p> <ol> <li>This is your first connection with the specific cluster.</li> <li>You have removed the cluster address entry from your computer\u2019s ~/.ssh/known_hosts file. This file contains the key fingerprints of remote systems you have previously connected to. When you start an SSH session, ssh checks the key fingerprint in this file to the one presented during an SSH connection attempt. If they are different, it will ask you to confirm the connection attempt.</li> <li>The servers ssh configurations have been updated, causing the key fingerprint to change. Before confirming the connection, contact the server administrators to confirm the update before continuing. The Alliance cluster key fingerprints are available for comparison on the technical documentation wiki SSH Security Improvements page.</li> <li>A bad actor has created an on-path cyberattack to capture your username and password for future use.</li> </ol> <p>Type <code>yes</code> to continue connecting to the remote system only if reasons 1, 2, or 3 explain why you are seeing the challenge message. </p> <p>When you type your password in, you will not see anything appear on the screen: your password will not be printed, nor will you see any key stroke indicators like dots or asterisks. This means you need to be extra careful typing your password - entering an incorrect password more than a few times will cause the system to block your connection attempts for some period of time before you can try again. </p> <p>Using the Alliance clusters.</p> <p>The Alliance clusters use multifactor authentication for logging in, meaning that you will need to have it set up before trying to connect to a cluster.</p>"},{"location":"terminal/#working-in-the-remote-system-shell","title":"Working in the remote system shell.","text":"<p>The first thing you will notice is a change to the prompt in your terminal, indicating that you are on a different system. Your prompt should now display your username, the remote system's hostname, the current working directory, and then a prompt symbol. </p> <pre><code>[finnsdot94@login1 ~]$ whoami\nfinnsdot94\n[finnsdot94@login1 ~]$ pwd\n/home/finnsdot94\n</code></pre> <p>Above, you can see that my name in this remote system is <code>finnsdot94</code> and that I am currently connected to the <code>login1</code> node (the first node on the cluster) and working in the highest-level working directory (my home directory).</p> <p>Now, let's see what else is in our working directory. To list the files and folders in your working directory, use the command <code>ls</code>. Since we haven't added any files or folders yet, you should just see the default <code>project</code> and <code>scratch</code> directories. </p> <p>The <code>ls</code> command can be modified to give you more informaiton with a number of options, such as <code>-a</code> which will include hidden entities, or <code>-l</code> which will provide a more detailed output. These options can also be combined, like below: </p> <p>For information about all the options available with a command, type the command followed by <code>--help</code> and hit enter.</p> <p>Finally, you can navigate between directories using the command <code>cd</code> which here means 'change directory'. To move into another directory, type the command <code>cd</code> followed by the cane of the directory you'd like to move into. Here, I'm using <code>cd</code> to move into <code>projects</code>:</p> <pre><code>[finnsdot94@login1 ~]$ cd projects\n[finnsdot94@login1 projects] $\n</code></pre> <p>You'll notice that now the wokring directory location in my prompt has changed from <code>~</code> to <code>projects</code>. You can move back up to the home directory by using the command <code>cd ..</code></p> <p>Navigating the file system.</p> <p>File systems in our computer are best thought of as hierarchical or nested systems. Our home working directory is the highest level, with every other directed nested inside. </p> <p>You can move across multiple levels with one <code>cd</code> command, so long as you specify the file path. So, to move down two levels, you would need to use <code>cd folder/subfolder</code> or to move up two levels, you'd use <code>cd ../..</code></p> <p>Exiting the remote system is easy: just type <code>exit</code> in the terminal and hit enter. </p>"}]}